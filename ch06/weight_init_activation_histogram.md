## 全体の流れ

1. **活性化関数を用意**

   * `sigmoid`, `ReLU`, `tanh` を定義している（実際に使うのは下の切り替え部分）。

2. **入力データを生成**

   * `input_data = np.random.randn(1000, 100)`
   * 1000サンプル、各サンプルは100次元（特徴量100）。
   * 以降、層を通していくデータ `x` は基本的に形状 `(1000, 100)` のまま進む。

3. **ネットワーク構造の指定**

   * `node_num = 100`：各隠れ層のノード数（ここでは100）
   * `hidden_layer_size = 5`：隠れ層5層
   * `activations = {}`：各層の出力 `z` を保存する辞書

4. **5層ぶん順伝播（forward）**

   * 最初は `x = input_data`
   * 各層 `i` でやっていることは同じで、ざっくり **線形変換 → 活性化**：

     1. **前の層の出力を入力にする**

        * `i != 0` のとき `x = activations[i-1]`
     2. **重み行列を初期化**

        * `w = np.random.randn(node_num, node_num) * 1`
        * 形状は `(100, 100)`（ノード→ノードの写像）
        * コメントアウトされている他の候補は、スケール違い（小さい値、Xavier、Heなど）を試すため。
     3. **線形変換**

        * `a = np.dot(x, w)`
        * `x: (1000,100)` と `w: (100,100)` なので `a: (1000,100)`
     4. **活性化関数**

        * `z = sigmoid(a)`（今はsigmoid）
        * `z` も `(1000,100)`
     5. **保存**

        * `activations[i] = z`

5. **各層の出力分布をヒストグラムで描画**

   * 各層の `a`（ここでは辞書に入ってる `z`）を `flatten()` して1次元化し、ヒストグラムにする。
   * `range=(0,1)` になっているので **sigmoid前提**の表示（sigmoidの出力は0〜1）。
   * 5つのサブプロットに、1層目〜5層目の分布が横並びで表示される。


